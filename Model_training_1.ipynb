{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "121dbd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "825381cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45054b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4571 images belonging to 4 classes.\n",
      "Found 1141 images belonging to 4 classes.\n",
      "Found 1311 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "img_size = (224, 224)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "\n",
    "train_ds = train_datagen.flow_from_directory(\n",
    "    './archive/Training',\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_ds = train_datagen.flow_from_directory(\n",
    "    './archive/Training',\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_ds = test_datagen.flow_from_directory(\n",
    "    './archive/Testing',\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e47bc710",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the convolutional layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b55d0f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers[-6:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "264beab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(base_model)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(4, activation='softmax'))  # 4 output classes\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#model.compile(optimizer=Adam(learning_rate=lr_schedule(0)), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a487bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               6422784   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21138500 (80.64 MB)\n",
      "Trainable params: 15863044 (60.51 MB)\n",
      "Non-trainable params: 5275456 (20.12 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3562d0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "142/142 [==============================] - 2361s 16s/step - loss: 0.6348 - accuracy: 0.7480 - val_loss: 0.7776 - val_accuracy: 0.7143\n",
      "Epoch 2/15\n",
      "142/142 [==============================] - 1502s 11s/step - loss: 0.3083 - accuracy: 0.8934 - val_loss: 0.4598 - val_accuracy: 0.8464\n",
      "Epoch 3/15\n",
      "142/142 [==============================] - 1500s 11s/step - loss: 0.2403 - accuracy: 0.9152 - val_loss: 0.4214 - val_accuracy: 0.8634\n",
      "Epoch 4/15\n",
      "142/142 [==============================] - 1499s 11s/step - loss: 0.1835 - accuracy: 0.9374 - val_loss: 0.4105 - val_accuracy: 0.8830\n",
      "Epoch 5/15\n",
      "142/142 [==============================] - 1504s 11s/step - loss: 0.1495 - accuracy: 0.9489 - val_loss: 0.4436 - val_accuracy: 0.8625\n",
      "Epoch 6/15\n",
      "142/142 [==============================] - 1501s 11s/step - loss: 0.1219 - accuracy: 0.9564 - val_loss: 0.3012 - val_accuracy: 0.9232\n",
      "Epoch 7/15\n",
      "142/142 [==============================] - 1500s 11s/step - loss: 0.1321 - accuracy: 0.9577 - val_loss: 0.3113 - val_accuracy: 0.9170\n",
      "Epoch 8/15\n",
      "142/142 [==============================] - 1501s 11s/step - loss: 0.1023 - accuracy: 0.9639 - val_loss: 0.2659 - val_accuracy: 0.9286\n",
      "Epoch 9/15\n",
      "142/142 [==============================] - 1500s 11s/step - loss: 0.0838 - accuracy: 0.9714 - val_loss: 0.3628 - val_accuracy: 0.9143\n",
      "Epoch 10/15\n",
      "142/142 [==============================] - 1502s 11s/step - loss: 0.0783 - accuracy: 0.9709 - val_loss: 0.2830 - val_accuracy: 0.9205\n",
      "Epoch 11/15\n",
      "142/142 [==============================] - 1523s 11s/step - loss: 0.0748 - accuracy: 0.9753 - val_loss: 0.2474 - val_accuracy: 0.9304\n",
      "Epoch 12/15\n",
      "142/142 [==============================] - 1508s 11s/step - loss: 0.0591 - accuracy: 0.9784 - val_loss: 0.3300 - val_accuracy: 0.9348\n",
      "Epoch 13/15\n",
      "142/142 [==============================] - 1503s 11s/step - loss: 0.0856 - accuracy: 0.9729 - val_loss: 0.4509 - val_accuracy: 0.8991\n",
      "Epoch 14/15\n",
      "142/142 [==============================] - 1504s 11s/step - loss: 0.0595 - accuracy: 0.9830 - val_loss: 0.2973 - val_accuracy: 0.9268\n",
      "Epoch 15/15\n",
      "142/142 [==============================] - 1502s 11s/step - loss: 0.0532 - accuracy: 0.9819 - val_loss: 0.2433 - val_accuracy: 0.9446\n"
     ]
    }
   ],
   "source": [
    "epochs = 15  # adjust as needed\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch=train_ds.samples // 32,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps=val_ds.samples // 32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "deccd5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 583s 14s/step - loss: 0.1663 - accuracy: 0.9657\n",
      "Test Accuracy: 96.57%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_ds)\n",
    "print(f'Test Accuracy: {test_acc * 100:.2f}%')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22543061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 1076s 26s/step - loss: 0.1663 - accuracy: 0.9657\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model \n",
    "model.save(\"model1.h5\") \n",
    "loaded_model = load_model(\"model1.h5\") \n",
    "loss, accuracy = loaded_model.evaluate(test_ds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b56a059",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (255496528.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[16], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    model_json = model.to_json(): # with open(\"network.json\", \"w\") as json_file:\u001b[0m\n\u001b[1;37m                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Saving model structure to a JSON file \n",
    "model_json = model.to_json()\n",
    "with open(\"model2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model_json = model.to_json() # with open(\"network.json\", \"w\") as json_file: \n",
    "    json_file.write(model_json) \n",
    "\n",
    "# Saving weights of the model to a HDF5 file \n",
    "model.save_weights(\"model2.h5\") \n",
    "\n",
    "# Loading JSON file \n",
    "json_file = open(\"network.json\", 'r') \n",
    "loaded_model_json = json_file.read() \n",
    "json_file.close() \n",
    "loaded_model = model_from_json(loaded_model_json) \n",
    "\n",
    "# Loading weights \n",
    "loaded_model.load_weights(\"network.h5\") \n",
    "loss, accuracy = loaded_model.evaluate(test_ds) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e54f23c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Method `model.to_yaml()` has been removed due to security risk of arbitrary code execution. Please use `model.to_json()` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Saving model structure to a YAML file \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model_yaml \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto_yaml() \n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetwork.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m yaml_file: \n\u001b[0;32m      4\u001b[0m \tyaml_file\u001b[38;5;241m.\u001b[39mwrite(model_yaml) \n",
      "File \u001b[1;32m~\\.conda\\envs\\vikas\\Lib\\site-packages\\keras\\src\\engine\\training.py:3302\u001b[0m, in \u001b[0;36mModel.to_yaml\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   3279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_yaml\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   3280\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a yaml string containing the network configuration.\u001b[39;00m\n\u001b[0;32m   3281\u001b[0m \n\u001b[0;32m   3282\u001b[0m \u001b[38;5;124;03m    Note: Since TF 2.6, this method is no longer supported and will raise a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3300\u001b[0m \u001b[38;5;124;03m        RuntimeError: announces that the method poses a security risk\u001b[39;00m\n\u001b[0;32m   3301\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   3303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod `model.to_yaml()` has been removed due to security risk of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marbitrary code execution. Please use `model.to_json()` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3305\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Method `model.to_yaml()` has been removed due to security risk of arbitrary code execution. Please use `model.to_json()` instead."
     ]
    }
   ],
   "source": [
    "# Saving model structure to a YAML file \n",
    "model_yaml = model.to_yaml() \n",
    "with open(\"network.yaml\", \"w\") as yaml_file: \n",
    "\tyaml_file.write(model_yaml) \n",
    "\n",
    "# Saving weights of the model to a HDF5 file \n",
    "model.save_weights(\"network.h5\") \n",
    "\n",
    "# Loading YAML file \n",
    "yaml_file = open(\"network.yaml\", 'r') \n",
    "loaded_model_yaml = yaml_file.read() \n",
    "yaml_file.close() \n",
    "loaded_model = model_from_yaml(loaded_model_yaml) \n",
    "\n",
    "# Loading weights \n",
    "loaded_model.load_weights(\"network.h5\") \n",
    "loss, accuracy = loaded_model.evaluate(test_ds) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9558f5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
